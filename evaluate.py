# K-IUM 2023. Code to evaluate participants' output.csv
# This script reports the AUROC regarding the presence of intracranial aneurysms
# and the overall accuracy (per location analysis).
# The required files:
#   (1) groundtruth.csv // will be used as a gold standard.
#   (2) output.csv // should be generated by the participants' model prior to running the script
# Note: omitted error handling for brevity.
# Hwangbo, Lee, MD. June 2023.
#
import pandas
from sklearn.metrics import roc_auc_score


def my_read_csv(filename):
    data = pandas.read_csv(filename)
    aneurysm = data.Aneurysm
    location = data.drop(columns=['Index', 'Aneurysm'])
    return aneurysm, location


def calc_accuracy(gt_location, output_location):
    confusion_tab = gt_location.subtract(output_location, axis='index', fill_value=0)
    confusion_tab = confusion_tab.abs()
    sum = confusion_tab.sum().sum()
    nrow, ncol = confusion_tab.shape
    total_elem = nrow * ncol
    a = 1.0 - (sum/total_elem)
    return a


def eval_model():
    gt_aneurysm, gt_location = my_read_csv('groundtruth.csv')
    output_aneurysm, output_location = my_read_csv('output.csv')
    auc_val = roc_auc_score(gt_aneurysm, output_aneurysm)
    acc_val = calc_accuracy(gt_location, output_location)
    return auc_val, acc_val
    # You may use evalute() if you need to evaluate your model


def main():
    auc, acc = eval_model()
    print('AUROC of the provided model')
    print (auc)
    print('Accuracy for locations')
    print (acc)


if __name__ == '__main__':
    main()